{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Spark Miniproject\n",
    "\n",
    "We'll use Spark to perform data manipulation, and analysis of a StackOverflow data set.   \n",
    "The data set can be found [here](https://archive.org/details/stackexchange).   \n",
    "We complete this project using the Python APIs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the data\n",
    "\n",
    "We are interested in the subfolders: allUsers, allPosts, and allVotes which contain chunked and gzipped xml with the following format:\n",
    "\n",
    "```\n",
    "<row Body=\"&lt;p&gt;I always validate my web pages, and I recommend you do the same BUT many large company websites DO NOT and cannot validate because the importance of the website looking exactly the same on all systems requires rules to be broken. &lt;/p&gt;&#10;&#10;&lt;p&gt;In general, valid websites help your page look good even on odd configurations (like cell phones) so you should always at least try to make it validate.&lt;/p&gt;&#10;\" CommentCount=\"0\" CreationDate=\"2008-10-12T20:26:29.397\" Id=\"195995\" LastActivityDate=\"2008-10-12T20:26:29.397\" OwnerDisplayName=\"Eric Wendelin\" OwnerUserId=\"25066\" ParentId=\"195973\" PostTypeId=\"2\" Score=\"0\" />\n",
    "```\n",
    "\n",
    "A full schema can be found [here](https://ia801500.us.archive.org/8/items/stackexchange/readme.txt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data input and parsing\n",
    "\n",
    "Discard rows that are split across multiple lines.  \n",
    "Malformatted XML can also be ignored.  \n",
    "It is enough to simply skip problematic rows, the loss of data will not significantly impact our results on this large data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bad_xml\n",
    "\n",
    "A simple question to test your parsing code. Create an RDD of Post objects where each Post is a valid row of XML from the Cross-Validated (stats.stackexchange.com) allPosts dataset.\n",
    "\n",
    "We are going to take several shortcuts to speed up and simplify our computations.  First, your parsing function to only attempt to parse rows that start with `  <row` as these denote actual data entries. This should be done in Spark as the data is being read in from disk, without any pre-Spark processing. \n",
    "\n",
    "Return the total number XML rows that started with ` <row` that were subsequently **rejected** during your processing.  Note that the text is unicode, and contains non-ascii characters.  You may need to re-encode to utf-8 (depending on your xml parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## upvote_percentage\n",
    "\n",
    "Each post on StackExchange can be upvoted, downvoted, and favorited. One \"sanity check\" we can do is to look at the ratio of upvotes to downvotes (referred to as \"UpMod\" and \"DownMod\" in the schema) as a function of how many times the post has been favorited.\n",
    "\n",
    "You might hypothesize, for example, that posts with more favorites should have a higher upvote/downvote ratio.\n",
    "\n",
    "Instead of looking at individual posts, we'll aggregate across number of favorites by using the post's number of favorites as our key. Since we're computing ratios, bundling together all posts with the same number of favorites effectively averages over them.  Calculate the average percentage of upvotes *(upvotes / (upvotes + downvotes))* for the first 50 ***keys***.\n",
    "\n",
    "Do the analysis on the smaller Cross-Validated dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## answer_percentage\n",
    "\n",
    "Investigate the correlation between a user's reputation and the kind of posts they make. For the 99 users with the highest reputation, single out posts which are either questions or answers and look at the percentage of these posts that are answers: *(answers / (answers + questions))*. \n",
    "\n",
    "Return a tuple of their **user ID** and this fraction.\n",
    "\n",
    "You should also return (-1, fraction) to represent the case where you average over all users (so you will return 100 entries total).\n",
    "\n",
    "Again, you only need to run this on the statistics overflow set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## post_counts\n",
    "\n",
    "If we use the total number of posts made on the site as a metric for tenure, we can look at the differences between \"younger\" and \"older\" users. You can imagine there might be many interesting features - for now just return the top 100 post counts among all users (of all types of posts) and the average reputation for every user who has that count.\n",
    "\n",
    "In other words, aggregate the cases where multiple users have the same post count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## quick_answers\n",
    "\n",
    "How long do you have to wait to get your question answered? Look at the set of ACCEPTED answers which are posted less than three hours after question creation. What is the average number of these \"quick answers\" as a function of the hour of day the question was asked? You should normalize by how many total accepted answers are garnered by questions posted in a given hour, just like we're counting how many quick accepted answers are garnered by questions posted in a given hour, eg. (quick accepted answers when question hour is 15 / total accepted answers when question hour is 15).\n",
    "\n",
    "Return a list, whose ith element correspond to ith hour (e.g. 0 -> midnight, 1 -> 1:00, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## quick_answers_full\n",
    "\n",
    "Same as above, but on the full StackExchange dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## identify_veterans\n",
    "\n",
    "It can be interesting to think about what factors influence a user to remain active on the site over a long period of time. In order not to bias the results towards older users, we'll define a time window between 100 and 150 days after account creation. If the user has made a post in this time, we'll consider them active and well on their way to being veterans of the site; if not, they are inactive and were likely brief users.\n",
    "\n",
    "*Consider*: What other parameterizations of \"activity\" could we use, and how would they differ in terms of splitting our user base?\n",
    "\n",
    "*Consider*: What other biases are still not dealt with, after using the above approach?\n",
    "\n",
    "Let's see if there are differences between the first ever question posts of \"veterans\" vs. \"brief users\". For each group separately, average the score, views, number of answers, and number of favorites of the users' **first question**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## identify_veterans_full\n",
    "\n",
    "Same as above, but on the full StackExchange dataset.\n",
    "\n",
    "No pre-parsed data is available for this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec\n",
    "\n",
    "Word2Vec is an alternative approach for vectorizing text data. The vectorized representations of words in the vocabulary tend to be useful for predicting other words in the document, hence the famous example \"vector('king') - vector('man') + vector('woman') ~= vector('queen')\".\n",
    "\n",
    "Let's see how good a Word2Vec model we can train using the tags of each StackExchange post as documents (this uses the full dataset). Use Spark ML's implementation of Word2Vec (this will require using DataFrames) to return a list of the top 25 closest synonyms to \"ggplot2\" and their similarity score in tuple format (\"string\", number).\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "The dimensionality of the vector space should be 100. The random seed should be 42L."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
