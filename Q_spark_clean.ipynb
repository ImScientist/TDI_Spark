{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Q1 (bad_xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip, sys, os, re\n",
    "from pyspark import SparkContext\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def localpath(path):\n",
    "    return str(os.path.abspath(os.path.curdir)) + path\n",
    "\n",
    "def startsWithRow(line):\n",
    "    return len( re.compile(u'^<row ').findall(line.strip()) ) > 0\n",
    "\n",
    "def startssEndsProperly(line):\n",
    "    return len( re.compile(u'(^<row(.*?)\\/>)').findall(line.strip()) ) > 0\n",
    "\n",
    "def startssEndsProperly_Throw_Trash(line):\n",
    "    return re.compile(u'(^<row(.*?)\\/>)').findall(line.strip())[0][0]\n",
    "\n",
    "def applyParser(line):\n",
    "    try:\n",
    "        root = ET.fromstring(re.compile(u'(^<row(.*?)\\/>)').findall(line.strip())[0][0].encode('utf-8').strip())\n",
    "        \n",
    "        return root\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local[*]\", \"temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines with posts: 109522\n",
      "readable lines: 108741\n",
      "unreadable posts: 781\n"
     ]
    }
   ],
   "source": [
    "my_input_dir = r'\\spark-stats-data\\allPosts\\part-0*'\n",
    "\n",
    "lines_1 = sc.textFile(localpath(my_input_dir)).map(startsWithRow).filter(lambda x: x is True).count()\n",
    "lines_2 = sc.textFile(localpath(my_input_dir)).map(applyParser).filter(lambda x: x is not None).count()\n",
    "lines_3 = lines_1-lines_2\n",
    "\n",
    "print \"lines with posts: %d\" % lines_1\n",
    "print \"readable lines: %d\" % lines_2\n",
    "print \"unreadable posts: %d\" % lines_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 (upvote_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Vote_El(object): #I get a pickle error\n",
    "    def __init__(self, PostId, Up, Down, Fav):\n",
    "        self.PostId = PostId\n",
    "        self.Up = Up\n",
    "        self.Down = Down\n",
    "        self.Fav = Fav\n",
    "    \n",
    "def Parse_Vote(line):\n",
    "    \n",
    "    root = applyParser(line)\n",
    "    \n",
    "    if root is not None:\n",
    "        \n",
    "        try:\n",
    "            PostId = int(root.get('PostId'))\n",
    "            Up, Down, Fav = 0, 0, 0\n",
    "        \n",
    "            var = int(root.get('VoteTypeId'))\n",
    "            if var == 2: Up += 1\n",
    "            elif var == 3: Down += 1\n",
    "            elif var == 5: Fav += 1\n",
    "        \n",
    "            #return Vote_El(PostId, Up, Down, Fav)\n",
    "            return (PostId, Up, Down, Fav)\n",
    "        \n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_input_dir = r'spark-stats-data\\allVotes\\part-0*'\n",
    "\n",
    "top_50 = sc.textFile(my_input_dir).map(Parse_Vote).filter(lambda x: x is not None).map(lambda x: (x[0], (x[1], x[2], x[3])) )\\\n",
    "    .reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1], x[2]+y[2]) ).map(lambda (key,val): (val[2], (val[0], val[1])) )\\\n",
    "    .reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1]) ).map(lambda (key,val): (key, val[0]/float(val[0]+val[1])  ) )\\\n",
    "    .sortByKey(ascending=True).take(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ4AAAD8CAYAAACGnEoDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSDAFh4gEU3CISDAFh4gE+3/HnPrXKUtcWAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9fb4518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = np.array([ [el[0], el[1]] for el in top_50 ])\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter( data[:,0],  data[:,1], alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3  (answer_percentage)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numbers\n",
    "\n",
    "def Get_Ustr_Info(line):\n",
    "    \n",
    "    root = applyParser(line)\n",
    "    \n",
    "    if root is not None:\n",
    "        try:    \n",
    "            UsrId = int(root.get('Id'))\n",
    "            Reputation = int(root.get('Reputation'))\n",
    "            return (UsrId, Reputation)\n",
    "        \n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def Get_Posts_Info(line):\n",
    "    \n",
    "    root = applyParser(line)\n",
    "    \n",
    "    if root is not None:\n",
    "        try:\n",
    "            \n",
    "            if 'OwnerUserId' in root.attrib:\n",
    "                UsrId = int(root.attrib['OwnerUserId'])\n",
    "            else:\n",
    "                UsrId = -2\n",
    "            \n",
    "            quest, answ = 0, 0\n",
    "            \n",
    "            var = int(root.get('PostTypeId')) \n",
    "            if var == 1: quest += 1\n",
    "            elif var == 2: answ += 1\n",
    "            \n",
    "            return (UsrId, (quest, answ))\n",
    "        except:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def Filter(el):\n",
    "    try:\n",
    "        key, val = el\n",
    "        if isinstance(key, numbers.Number) and isinstance(val[1], numbers.Number) and isinstance(val[0][0], numbers.Number) and isinstance(val[0][1], numbers.Number):\n",
    "            return True\n",
    "        else:\n",
    "            False\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_input_Posts = r'spark-stats-data\\allPosts\\part-0*'\n",
    "my_input_Users = r'spark-stats-data\\allUsers\\part-0*'\n",
    "\n",
    "posts = sc.textFile(my_input_Posts).map(Get_Posts_Info).filter(lambda x: x is not None)\n",
    "users = sc.textFile(my_input_Users).map(Get_Ustr_Info).filter(lambda x: x is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table = posts.join(users).filter(Filter).map(lambda (key,val): (key, (val[1], val[0][0], val[0][1])) )\\\n",
    "        .reduceByKey(lambda x,y: (x[0], x[1]+y[1], x[2]+y[2]) ).filter(lambda (key,val): val[1]+val[2]!=0 )\\\n",
    "        .map(lambda (key,val): (val[0], val[2]/float(val[1]+val[2])) ).sortByKey(ascending=False)\\\n",
    "        .take(99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(52060, 55304, 0.5151074848180023)]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_ = posts.map(lambda (k,v): (1, (v[0], v[1])) )\\\n",
    "        .reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1]) )\\\n",
    "        .map(lambda (k,v): (v[0], v[1], v[1]/float(v[0]+v[1])) )\\\n",
    "        .collect()\n",
    "\n",
    "avg_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4 (post_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lxml import objectify, etree\n",
    "import re\n",
    "\n",
    "def parse_Posts_trf_3(line):    \n",
    "    try:\n",
    "        strings = re.search('^<row', line.encode('utf-8').strip())\n",
    "        if strings:\n",
    "            root = objectify.fromstring(line.encode('utf-8').strip())\n",
    "            \n",
    "            if 'OwnerUserId' in root.attrib:\n",
    "                OwnerUserId = int(root.attrib['OwnerUserId'])\n",
    "            else:\n",
    "                OwnerUserId = -2\n",
    "                \n",
    "            return (OwnerUserId, 1)\n",
    "        return None\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_input_dir = r'spark-stats-data/allPosts/part-0*'\n",
    "\n",
    "Own_Id_vs_QA = sc.textFile(localpath(my_input_dir))\\\n",
    ".map(parse_Posts_trf_3)\\\n",
    ".filter(lambda x: x is not None)\\\n",
    ".reduceByKey(lambda va, vb: va+vb)\n",
    "\n",
    "\n",
    "my_input_dir = r'spark-stats-data/allUsers/part-0*'\n",
    "\n",
    "Usr_Id_vs_Rep = sc.textFile(localpath(my_input_dir))\\\n",
    ".map(parse_User_Record_trf_1)\\\n",
    ".filter(lambda x: x is not None)\n",
    "\n",
    "\n",
    "Usr_Id_vs_Rep_vs_QA = Usr_Id_vs_Rep.join(Own_Id_vs_QA)\\\n",
    ".map(lambda (usr_id,(rep,qa)): (qa, (rep,1)) )\\\n",
    ".reduceByKey(lambda va, vb: (va[0]+vb[0], va[1]+vb[1]))\\\n",
    ".map(lambda (qa, (rep_sum, counts)):  (qa, float(rep_sum)/counts) )\\\n",
    ".sortBy(lambda (qa, avg_rep): qa, ascending=False)\\\n",
    ".collect()\n",
    "\n",
    "q4_res = Usr_Id_vs_Rep_vs_QA[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5 (quick_answers)\n",
    "        Id\n",
    "        PostTypeId\n",
    "          - 1: Question\n",
    "          - 2: Answer\n",
    "       - ParentID (only present if PostTypeId is 2)\n",
    "       - AcceptedAnswerId (only present if PostTypeId is 1)\n",
    "       - CreationDate\n",
    "       \n",
    "questoins: (Q_id, acc_answer_id, creation_date)  \n",
    "answer:    (A_id, ParentID, creation_date  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lxml import objectify, etree\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def parse_Posts_trf_2(line):    \n",
    "    try:\n",
    "        strings = re.search('^<row', line.encode('utf-8').strip())\n",
    "        if strings:\n",
    "            root = objectify.fromstring(line.encode('utf-8').strip())\n",
    "            \n",
    "            if 'Id' in root.attrib:\n",
    "                Id = int(root.attrib['Id'])\n",
    "            else:\n",
    "                Id = -2\n",
    "            \n",
    "            #or just return None if CreationDate is not given\n",
    "            if 'CreationDate' in root.attrib:\n",
    "                CreationDate = datetime.strptime(root.attrib['CreationDate'], '%Y-%m-%dT%H:%M:%S.%f').replace(microsecond=0)\n",
    "            else:\n",
    "                CreationDate = datetime.datetime(1970, 1, 1, 0, 0, 0)\n",
    "            \n",
    "            PostTypeId = int(root.attrib['PostTypeId'])\n",
    "            if PostTypeId == 2:\n",
    "                if 'ParentID' in root.attrib:\n",
    "                    ParentID         = int(root.attrib['ParentID'])\n",
    "                else:\n",
    "                    ParentID         = -2\n",
    "                return (Id, ParentID, CreationDate, 'A')\n",
    "                \n",
    "            if PostTypeId == 1:\n",
    "                if 'AcceptedAnswerId' in root.attrib:\n",
    "                    AcceptedAnswerId = int(root.attrib['AcceptedAnswerId'])\n",
    "                else:\n",
    "                    AcceptedAnswerId = -2\n",
    "                return (Id, AcceptedAnswerId, CreationDate, 'Q')\n",
    "\n",
    "            return None\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_input_dir = r'spark-stats-data/allPosts/part-0*'\n",
    "\n",
    "q_with_acc_answ = sc.textFile(localpath(my_input_dir))\\\n",
    ".map(parse_Posts_trf_2)\\\n",
    ".filter(lambda x: x is not None)\\\n",
    ".filter(lambda x: x[0]!=-2 and x[1]!=-2 and x[2]!=datetime(1970, 1, 1, 0, 0, 0) and x[3]=='Q')\n",
    "\n",
    "\n",
    "answ = sc.textFile(localpath(my_input_dir))\\\n",
    ".map(parse_Posts_trf_2)\\\n",
    ".filter(lambda x: x is not None)\\\n",
    ".filter(lambda x: x[0]!=-2 and              x[2]!=datetime(1970, 1, 1, 0, 0, 0) and x[3]=='A')\n",
    "\n",
    "\n",
    "acc_answ_in_my_set = q_with_acc_answ.map(lambda x: (x[1], x[2]) ).join( answ.map(lambda x: (x[0],x[2]) ) )\n",
    "\n",
    "quick_acc_answ_in_my_set = acc_answ_in_my_set.filter(lambda x: x[1][1]-x[1][0]<timedelta(0, 10800) )\n",
    "\n",
    "\n",
    "\n",
    "tot_acc_answ_by_q_hour = acc_answ_in_my_set.map(lambda x: (x[1][0].hour ,1) ).reduceByKey(lambda x, y: x+y).sortByKey().collect()\n",
    "\n",
    "tot_quick_acc_answ_by_hour = quick_acc_answ_in_my_set.map(lambda x: (x[1][0].hour ,1) ).reduceByKey(lambda x, y: x+y).sortByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q5_res = []\n",
    "for el1, el2 in zip(tot_acc_answ_by_q_hour, tot_quick_acc_answ_by_hour):\n",
    "    q5_res.append(float(el2[1])/el1[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6 (quick_answers_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_input_dir = r'spark-stack-data/allPosts/part-0*'\n",
    "\n",
    "q_with_acc_answ = sc.textFile(localpath(my_input_dir))\\\n",
    ".map(parse_Posts_trf_2)\\\n",
    ".filter(lambda x: x is not None)\\\n",
    ".filter(lambda x: x[0]!=-2 and x[1]!=-2 and x[2]!=datetime(1970, 1, 1, 0, 0, 0) and x[3]=='Q')\n",
    "\n",
    "\n",
    "answ = sc.textFile(localpath(my_input_dir))\\\n",
    ".map(parse_Posts_trf_2)\\\n",
    ".filter(lambda x: x is not None)\\\n",
    ".filter(lambda x: x[0]!=-2 and              x[2]!=datetime(1970, 1, 1, 0, 0, 0) and x[3]=='A')\n",
    "\n",
    "\n",
    "# id_acc_answ, cr_date_Q, cr_data_A\n",
    "acc_answ_in_my_set = q_with_acc_answ.map(lambda x: (x[1], x[2]) ).join( answ.map(lambda x: (x[0],x[2]) ) )\n",
    "\n",
    "# id_acc_answ, cr_date_Q, cr_data_A\n",
    "quick_acc_answ_in_my_set = acc_answ_in_my_set.filter(lambda x: x[1][1]-x[1][0]<timedelta(0, 10800) )\n",
    "\n",
    "\n",
    "\n",
    "tot_acc_answ_by_q_hour = acc_answ_in_my_set.map(lambda x: (x[1][0].hour ,1) ).reduceByKey(lambda x, y: x+y).sortByKey().collect()\n",
    "\n",
    "tot_quick_acc_answ_by_hour = quick_acc_answ_in_my_set.map(lambda x: (x[1][0].hour ,1) ).reduceByKey(lambda x, y: x+y).sortByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q6_res = []\n",
    "for el1, el2 in zip(tot_acc_answ_by_q_hour, tot_quick_acc_answ_by_hour):\n",
    "    q6_res.append(float(el2[1])/el1[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7 (identify_veterans)\n",
    "posts 100-150 days after account creation date -> (active vs inactive users)  \n",
    "get first question of veterans and users  \n",
    "for both groups: average the score, views, number of answers and number of favorites of the users' first question  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PostTypeId: 1-Q, 2-A get something only if PostTypeId == 1\n",
    "from lxml import objectify, etree\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def parse_User_Record_trf_2(line):\n",
    "    try:\n",
    "        strings = re.search(r'^<row', line.strip().encode('utf-8'))\n",
    "        if strings:\n",
    "            root = objectify.fromstring(line.strip().encode('utf-8'))\n",
    "        \n",
    "            if 'Id' in root.attrib: Id = int(root.attrib['Id'])\n",
    "            else: Id = -2\n",
    "    \n",
    "            if 'CreationDate' in root.attrib: CreationDate = datetime.strptime(root.attrib['CreationDate'], '%Y-%m-%dT%H:%M:%S.%f').replace(microsecond=0)\n",
    "            else: CreationDate = datetime.datetime(1970, 1, 1, 0, 0, 0)\n",
    "        \n",
    "            return (Id, CreationDate)\n",
    "        return None\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def parse_Posts_trf_4(line):\n",
    "    try:\n",
    "        strings = re.search(r'^<row', line.strip().encode('utf-8'))\n",
    "        if strings:\n",
    "            root = objectify.fromstring(line.strip().encode('utf-8'))\n",
    "            \n",
    "            if 'OwnerUserId' in root.attrib: OwnerUserId = int(root.attrib['OwnerUserId'])\n",
    "            else: OwnerUserId = -2\n",
    "            \n",
    "            if 'PostTypeId' in root.attrib: PostTypeId = int(root.attrib['PostTypeId'])\n",
    "            else: PostTypeId = -2\n",
    "            \n",
    "            if 'CreationDate' in root.attrib: CreationDate = datetime.strptime(root.attrib['CreationDate'], '%Y-%m-%dT%H:%M:%S.%f').replace(microsecond=0)\n",
    "            else: CreationDate = datetime.datetime(1970, 1, 1, 0, 0, 0)\n",
    "            \n",
    "            if 'Score' in root.attrib: Score = int(root.attrib['Score'])\n",
    "            else: Score = 0 #-999999\n",
    "            \n",
    "            if 'ViewCount' in root.attrib: ViewCount = int(root.attrib['ViewCount'])\n",
    "            else: ViewCount = 0 #-2\n",
    "            \n",
    "            if 'AnswerCount' in root.attrib: AnswerCount = int(root.attrib['AnswerCount'])\n",
    "            else: AnswerCount = 0 #-2\n",
    "            \n",
    "            if 'FavoriteCount' in root.attrib: FavoriteCount = int(root.attrib['FavoriteCount'])\n",
    "            else: FavoriteCount = 0 #-2\n",
    "            \n",
    "            return (OwnerUserId, PostTypeId, CreationDate, Score, ViewCount, AnswerCount, FavoriteCount)\n",
    "            \n",
    "        return None\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_input_dir = r'spark-stats-data/allUsers/part-0*'\n",
    "Usr_Id_vs_Cr_date = sc.textFile(localpath(my_input_dir)).map(parse_User_Record_trf_2).filter(lambda x: x is not None and x[1]!=datetime(1970, 1, 1, 0, 0, 0))\n",
    "\n",
    "\n",
    "my_input_dir = r'spark-stats-data/allPosts/part-0*'\n",
    "all_posts = sc.textFile(localpath(my_input_dir)).map(parse_Posts_trf_4).filter(lambda x: x is not None and x[2]!=datetime(1970, 1, 1, 0, 0, 0))\n",
    "\n",
    "\n",
    "first_q_by_usr = sc.textFile(localpath(my_input_dir)).map(parse_Posts_trf_4)\\\n",
    ".filter(lambda x: x is not None and x[0]!=-2 and x[1]==1 and x[2]!=datetime(1970, 1, 1, 0, 0, 0) and x[3]!=-999999 and x[4]!=-2 and x[5]!=-2 and x[6]!=-2 )\\\n",
    ".map(lambda x: (x[0], (x[1],x[2],x[3],x[4],x[5],x[6]) ))\\\n",
    ".reduceByKey(lambda x, y: x if x[1]<y[1] else y )\n",
    "\n",
    "\n",
    "act_usr = Usr_Id_vs_Cr_date.join(  all_posts.map(lambda x: (x[0], x[2]))  )\\\n",
    ".map(lambda x: (x[0], 1) if timedelta(100, 0)<x[1][1]-x[1][0]<timedelta(150, 0) else (x[0], 0) )\\\n",
    ".reduceByKey(lambda x, y: x+y)\\\n",
    ".filter(lambda x: x[1]>0)\\\n",
    ".map(lambda x: (x[0],1) )\n",
    "\n",
    "\n",
    "inact_usr = Usr_Id_vs_Cr_date.join(  all_posts.map(lambda x: (x[0], x[2]))  )\\\n",
    ".map(lambda x: (x[0], 1) if timedelta(100, 0)<x[1][1]-x[1][0]<timedelta(150, 0) else (x[0], 0) )\\\n",
    ".reduceByKey(lambda x, y: x+y)\\\n",
    ".filter(lambda x: x[1]==0)\\\n",
    ".map(lambda x: (x[0],1) )\n",
    "\n",
    "\n",
    "act_usr_first_q_stats = act_usr.join(first_q_by_usr)\\\n",
    ".map(lambda x: (x[1][1][2],x[1][1][3],x[1][1][4],x[1][1][5],1))\\\n",
    ".reduce(lambda x, y: (x[0]+y[0], x[1]+y[1], x[2]+y[2], x[3]+y[3], x[4]+y[4]) )\n",
    "\n",
    "inact_usr_first_q_stats = inact_usr.join(first_q_by_usr)\\\n",
    ".map(lambda x: (x[1][1][2],x[1][1][3],x[1][1][4],x[1][1][5],1))\\\n",
    ".reduce(lambda x, y: (x[0]+y[0], x[1]+y[1], x[2]+y[2], x[3]+y[3], x[4]+y[4]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def identify_veterans():\n",
    "    return {\"vet_score\": float(act_usr_first_q_stats[0])/act_usr_first_q_stats[4],\n",
    "            \"vet_views\": float(act_usr_first_q_stats[1])/act_usr_first_q_stats[4],\n",
    "            \"vet_answers\": float(act_usr_first_q_stats[2])/act_usr_first_q_stats[4],\n",
    "            \"vet_favorites\": float(act_usr_first_q_stats[3])/act_usr_first_q_stats[4],\n",
    "            \"brief_score\": float(inact_usr_first_q_stats[0])/inact_usr_first_q_stats[4],\n",
    "            \"brief_views\": float(inact_usr_first_q_stats[1])/inact_usr_first_q_stats[4],\n",
    "            \"brief_answers\": float(inact_usr_first_q_stats[2])/inact_usr_first_q_stats[4],\n",
    "            \"brief_favorites\": float(inact_usr_first_q_stats[3])/inact_usr_first_q_stats[4]\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q8 (identify_veterans_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_input_dir = r'spark-stack-data/allUsers/part-0*'\n",
    "Usr_Id_vs_Cr_date = sc.textFile(localpath(my_input_dir)).map(parse_User_Record_trf_2).filter(lambda x: x is not None and x[1]!=datetime(1970, 1, 1, 0, 0, 0))\n",
    "\n",
    "\n",
    "my_input_dir = r'spark-stack-data/allPosts/part-0*'\n",
    "all_posts = sc.textFile(localpath(my_input_dir)).map(parse_Posts_trf_4).filter(lambda x: x is not None and x[2]!=datetime(1970, 1, 1, 0, 0, 0))\n",
    "\n",
    "\n",
    "first_q_by_usr = sc.textFile(localpath(my_input_dir)).map(parse_Posts_trf_4)\\\n",
    ".filter(lambda x: x is not None and x[0]!=-2 and x[1]==1 and x[2]!=datetime(1970, 1, 1, 0, 0, 0) and x[3]!=-999999 and x[4]!=-2 and x[5]!=-2 and x[6]!=-2 )\\\n",
    ".map(lambda x: (x[0], (x[1],x[2],x[3],x[4],x[5],x[6]) ))\\\n",
    ".reduceByKey(lambda x, y: x if x[1]<y[1] else y )\n",
    "\n",
    "\n",
    "act_usr = Usr_Id_vs_Cr_date.join(  all_posts.map(lambda x: (x[0], x[2]))  )\\\n",
    ".map(lambda x: (x[0], 1) if timedelta(100, 0)<x[1][1]-x[1][0]<timedelta(150, 0) else (x[0], 0) )\\\n",
    ".reduceByKey(lambda x, y: x+y)\\\n",
    ".filter(lambda x: x[1]>0)\\\n",
    ".map(lambda x: (x[0],1) )\n",
    "\n",
    "\n",
    "inact_usr = Usr_Id_vs_Cr_date.join(  all_posts.map(lambda x: (x[0], x[2]))  )\\\n",
    ".map(lambda x: (x[0], 1) if timedelta(100, 0)<x[1][1]-x[1][0]<timedelta(150, 0) else (x[0], 0) )\\\n",
    ".reduceByKey(lambda x, y: x+y)\\\n",
    ".filter(lambda x: x[1]==0)\\\n",
    ".map(lambda x: (x[0],1) )\n",
    "\n",
    "\n",
    "act_usr_first_q_stats = act_usr.join(first_q_by_usr)\\\n",
    ".map(lambda x: (x[1][1][2],x[1][1][3],x[1][1][4],x[1][1][5],1))\\\n",
    ".reduce(lambda x, y: (x[0]+y[0], x[1]+y[1], x[2]+y[2], x[3]+y[3], x[4]+y[4]) )\n",
    "\n",
    "inact_usr_first_q_stats = inact_usr.join(first_q_by_usr)\\\n",
    ".map(lambda x: (x[1][1][2],x[1][1][3],x[1][1][4],x[1][1][5],1))\\\n",
    ".reduce(lambda x, y: (x[0]+y[0], x[1]+y[1], x[2]+y[2], x[3]+y[3], x[4]+y[4]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def identify_veterans_full():\n",
    "    return {\"vet_score\": float(act_usr_first_q_stats[0])/act_usr_first_q_stats[4],\n",
    "            \"vet_views\": float(act_usr_first_q_stats[1])/act_usr_first_q_stats[4],\n",
    "            \"vet_answers\": float(act_usr_first_q_stats[2])/act_usr_first_q_stats[4],\n",
    "            \"vet_favorites\": float(act_usr_first_q_stats[3])/act_usr_first_q_stats[4],\n",
    "            \"brief_score\": float(inact_usr_first_q_stats[0])/inact_usr_first_q_stats[4],\n",
    "            \"brief_views\": float(inact_usr_first_q_stats[1])/inact_usr_first_q_stats[4],\n",
    "            \"brief_answers\": float(inact_usr_first_q_stats[2])/inact_usr_first_q_stats[4],\n",
    "            \"brief_favorites\": float(inact_usr_first_q_stats[3])/inact_usr_first_q_stats[4]\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q9 (word2vec)\n",
    "using the tags of each StackExchange post as documents  \n",
    "Spark ML's implementation of Word2Vec (this will require using DataFrames)  \n",
    "list of the top 25 closest synonyms to \"ggplot2\" and their similarity score in tuple format (\"string\", number)  \n",
    "dimensionality of the vector space should be 100. The random seed should be 42L."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local[*]\", \"temp\")\n",
    "\n",
    "import os\n",
    "def localpath(path):\n",
    "    return 'file://' + str(os.path.abspath(os.path.curdir)) + '/' + path\n",
    "\n",
    "# needed to convert RDDs into DataFrames\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "from pyspark.ml.feature import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lxml import objectify, etree\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def parse_Posts_trf_5(line):    \n",
    "    try:\n",
    "        strings = re.search('^<row', line.encode('utf-8').strip())\n",
    "        if strings:\n",
    "            root = objectify.fromstring(line.encode('utf-8').strip())\n",
    "            Tags = re.compile('\\w+').findall( root.attrib['Tags'] )\n",
    "            return (Tags)\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_input_dir = r'spark-stack-data/allPosts/part*'\n",
    "\n",
    "my_tags = sc.textFile(localpath(my_input_dir)).map(parse_Posts_trf_5).filter(lambda x: x is not None).map(lambda x: (x,1)).toDF(['tags', 'some_var'])\n",
    "w2v = Word2Vec(vectorSize=100, seed=42, inputCol='tags', outputCol='vectors')#, minCount=1\n",
    "my_model = w2v.fit(my_tags)\n",
    "\n",
    "vectors = my_model.getVectors().rdd.map(lambda x: (x.word, x.vector))\n",
    "\n",
    "n_synonyms = 25\n",
    "my_synonyms = my_model.findSynonyms(\"ggplot2\", n_synonyms).select(\"word\", \"similarity\").take(n_synonyms)\n",
    "Q9_res = []\n",
    "for i in range(n_synonyms):\n",
    "    Q9_res.append( (my_synonyms[i][0], my_synonyms[i][1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
